{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "from sklearn import metrics\n",
    "import sklearn.metrics as sm\n",
    "from sklearn import manifold\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "\n",
    "from hazm import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import csv, re, pickle\n",
    "\n",
    "from colorama import Back, Fore, Style\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pyclustering as pyclus\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"dataset.xlsx\")\n",
    "reviews = data['question']\n",
    "# rate = data['Score']\n",
    "labels = list(data['subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "RE_USELESS = r'[^\\w]'  # remove useless characters\n",
    "RE_DIGIT = r\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\"  # remove digits\n",
    "RE_SPACE = r'\\s+'  # remove space\n",
    "RE_EMAILS = r'[\\w\\.-]+@[\\w\\.-]+'\n",
    "RE_URLS = r'http\\S+'\n",
    "RE_WWW = r'www\\S+'\n",
    "\n",
    "\n",
    "def clean_all_save(document, save_file_path):\n",
    "    \"\"\"\n",
    "    this function generate raw persian text, it remove non-persian character\n",
    "    and all numbers and symbols\n",
    "    :param document:\n",
    "    :param save_file_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    with open(save_file_path, 'w') as output:\n",
    "        for sentence in document:\n",
    "            sentence = clean_sentence(sentence)\n",
    "            output.write(sentence + '\\n')\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_all(document, doc_pattern=r'<TEXT>(.*?)</TEXT>'):\n",
    "    \"\"\"\n",
    "    clean text like hamshahri, irBlogs, and other Treck format\n",
    "    :param document:\n",
    "    :param doc_pattern:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    clean = ''\n",
    "    document = re.findall(doc_pattern, document, re.DOTALL)\n",
    "    for sentence in document:\n",
    "        sentence = clean_sentence(sentence)\n",
    "        clean += ' \\n' + sentence\n",
    "    return clean\n",
    "\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = re.sub(r'[^\\u0621-\\u06ff]', ' ', sentence)\n",
    "    sentence = arToPersianChar(sentence)\n",
    "    sentence = arToPersianNumb(sentence)\n",
    "    sentence = faToEnglishNumb(sentence)\n",
    "    sentence = re.sub(r'[a-zA-Z]', ' ', sentence)\n",
    "    sentence = re.sub(r'[0-9]', ' ', sentence)\n",
    "    sentence = re.sub(RE_WWW, r' ', sentence)\n",
    "    sentence = re.sub(RE_URLS, r' ', sentence)\n",
    "    sentence = re.sub(RE_EMAILS, r' ', sentence)\n",
    "    sentence = re.sub(RE_USELESS, r' ', sentence)\n",
    "    sentence = re.sub(RE_DIGIT, r' ', sentence)\n",
    "    sentence = re.sub(RE_SPACE, r' ', sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def arToPersianNumb(number):\n",
    "    dic = {\n",
    "        '١': '۱',\n",
    "        '٢': '۲',\n",
    "        '٣': '۳',\n",
    "        '٤': '۴',\n",
    "        '٥': '۵',\n",
    "        '٦': '۶',\n",
    "        '٧': '۷',\n",
    "        '٨': '۸',\n",
    "        '٩': '۹',\n",
    "        '٠': '۰',\n",
    "    }\n",
    "    return multiple_replace(dic, number)\n",
    "\n",
    "\n",
    "def arToPersianChar(userInput):\n",
    "    dic = {\n",
    "        'ك': 'ک',\n",
    "        'دِ': 'د',\n",
    "        'بِ': 'ب',\n",
    "        'زِ': 'ز',\n",
    "        'ذِ': 'ذ',\n",
    "        'شِ': 'ش',\n",
    "        'سِ': 'س',\n",
    "        'ى': 'ی',\n",
    "        'ي': 'ی'\n",
    "    }\n",
    "    return multiple_replace(dic, userInput)\n",
    "\n",
    "\n",
    "def faToEnglishNumb(number):\n",
    "    dic = {\n",
    "        '۰': '0',\n",
    "        '۱': '1',\n",
    "        '۲': '2',\n",
    "        '۳': '3',\n",
    "        '۴': '4',\n",
    "        '۵': '5',\n",
    "        '۶': '6',\n",
    "        '۷': '7',\n",
    "        '۸': '8',\n",
    "        '۹': '9',\n",
    "    }\n",
    "    return multiple_replace(dic, number)\n",
    "\n",
    "\n",
    "def multiple_replace(dic, text):\n",
    "    pattern = \"|\".join(map(re.escape, dic.keys()))\n",
    "    return re.sub(pattern, lambda m: dic[m.group()], str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all(document):\n",
    "    clean = ''\n",
    "    for sentence in document:\n",
    "        sentence = clean_sentence(sentence)\n",
    "        clean += sentence\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = k = i = 0\n",
    "reviews1 = []\n",
    "labels1 = []\n",
    "# labels1 = list(labels.copy())\n",
    "normalizer = Normalizer()\n",
    "for review in reviews:\n",
    "    sentences = sent_tokenize(normalizer.normalize(clean_all(review)))\n",
    "    reviews1.extend(sentences)\n",
    "    for j in range(len(sentences)):\n",
    "        labels1.insert(i + k, labels[i])\n",
    "        k += 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews1),len(labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning dataset\n",
    "words=[]\n",
    "all_text = ''\n",
    "# stemmer = Stemmer()\n",
    "for t in range (len(reviews1)):\n",
    "    text = reviews1[t]\n",
    "    text = text.replace('\\u200c',' ')\n",
    "    text = text.replace('\\u200f',' ')\n",
    "    text = re.sub(r'[^a-zA-Z0-9آ-ی۰-۹ ]', ' ', text)\n",
    "    all_text += text\n",
    "    all_text += ' '\n",
    "    wordsInText = text.split()\n",
    "    for word in wordsInText:\n",
    "#         word = stemmer.stem(word)\n",
    "        if word != ' ' or word != '':\n",
    "            words.append(word)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "with open(\"mySavedDict.txt\", \"wb\") as myFile:\n",
    "    pickle.dump(vocab_to_int, myFile)\n",
    "\n",
    "'''\n",
    "with open(\"mySavedDict.txt\", \"rb\") as myFile:\n",
    "    myNewPulledInDictionary = pickle.load(myFile)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints = []\n",
    "for each in reviews1:\n",
    "    #print (each)\n",
    "    each = each.replace('\\u200c',' ')\n",
    "    each = each.replace('\\u200f',' ')\n",
    "    each = re.sub(r'[^a-zA-Z0-9آ-ی۰-۹ ]', ' ', each)\n",
    "    reviews_ints.append([vocab_to_int[word] for word in each.split()])\n",
    "\n",
    "\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = 15000\n",
    "su = ma = 0\n",
    "i = 0\n",
    "low = []\n",
    "for each in reviews_ints:\n",
    "    if len(each) == 2 or len(each) == 1:\n",
    "        low.append(i)\n",
    "    if len(each) <= mi:\n",
    "#         print(each,i,len(each))\n",
    "        mi = len(each)\n",
    "    if len(each) > ma:\n",
    "        ma = len(each)\n",
    "    su += len(each)\n",
    "    i += 1\n",
    "print('min lenght: '+str(mi),' and max lenght: '+str(ma),' and mean lenght: '+str(su/len(reviews_ints)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints22 = reviews_ints.copy()\n",
    "for i in range(len(low)):\n",
    "    print(reviews_ints22.pop(low[len(low)- i -1]),low[len(low)- i -1])\n",
    "len(reviews_ints22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints = reviews_ints22.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "X = np.zeros((len(reviews_ints), seq_len), dtype=int)\n",
    "\n",
    "for i, row in enumerate(reviews_ints):\n",
    "#     print (i , row)\n",
    "#     print (i )\n",
    "#     print ('****')\n",
    "    X[i, -len(row):] = np.array(row)[:seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NORMALIZATION = True\n",
    "\n",
    "\n",
    "def readVars(config_file):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "    budget = int(config.get(\"vars\", \"budget\"))\n",
    "    kmax = int(config.get(\"vars\", \"kmax\"))  # Maximum number of Clusters\n",
    "    numOfInd = int(config.get(\"vars\", \"numOfInd\"))  # number of individual\n",
    "    Ps = float(config.get(\"vars\", \"Ps\"))\n",
    "    Pm = float(config.get(\"vars\", \"Pm\"))\n",
    "    Pc = float(config.get(\"vars\", \"Pc\"))\n",
    "\n",
    "    return budget, kmax, Ps, Pm, Pc, numOfInd\n",
    "\n",
    "\n",
    "# minmax normalization\n",
    "def minmax(data):\n",
    "    normData = data\n",
    "    data = data.astype(float)\n",
    "    normData = normData.astype(float)\n",
    "    for i in range(0, data.shape[1]):\n",
    "        tmp = data.iloc[:, i]\n",
    "        # max of each column\n",
    "        maxElement = np.amax(tmp)\n",
    "        # min of each column\n",
    "        minElement = np.amin(tmp)\n",
    "\n",
    "        # norm_dat.shape[0] : size of row\n",
    "        for j in range(0, normData.shape[0]):\n",
    "            normData[i][j] = float(\n",
    "                data[i][j] - minElement) / (maxElement - minElement)\n",
    "\n",
    "    normData.to_csv('result/norm_data.csv', index=None, header=None)\n",
    "    return normData\n",
    "data = pd.DataFrame(X)\n",
    "data = minmax(data)  # normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:,:].values\n",
    "\n",
    "dataPoints=X.shape[0]\n",
    "features=X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #Preprocessing\n",
    "# ordinalEncoder=preprocessing.OrdinalEncoder()\n",
    "# ordinalList=[ordinalEncoder for i in range(dataPoints)]\n",
    "# for feature in range(X.shape[1]):\n",
    "#     o=preprocessing.OrdinalEncoder()\n",
    "#     missingValueImputer=SimpleImputer(missing_values=np.nan,strategy=\"constant\", fill_value=0)\n",
    "#     X[:,feature]=missingValueImputer.fit_transform(X[:,feature].reshape(-1,1)).reshape(-1,)\n",
    "#     X[:,feature]=o.fit_transform(X[:,feature].reshape(-1,1)).reshape(-1,)\n",
    "#     ordinalList[feature]=o;\n",
    "\n",
    "# missingValueImputer=SimpleImputer(missing_values=np.nan,strategy=\"constant\", fill_value=0)\n",
    "# X=missingValueImputer.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##apply WOA-kmeans clustering\n",
    "\n",
    "numberOfCluster=3\n",
    "numberOfWhale=10\n",
    "iterations=40\n",
    "# dataPoints=X.shape[0]\n",
    "# features=X.shape[1];\n",
    "#intialise\n",
    "\n",
    "centresOfwhale=np.zeros((numberOfWhale,numberOfCluster,features))\n",
    "\n",
    "\n",
    "for whale in range(numberOfWhale):\n",
    "    for cluster in range(numberOfCluster):\n",
    "        for feature in range(features):\n",
    "            centresOfwhale[whale,cluster,feature]=float(random.randint(np.min(X[:,feature]),np.max(X[:,feature])))\n",
    "bestWhale=0\n",
    "for iteration in range(iterations):\n",
    "    print(iteration)\n",
    "    #dataPointsInCluster=[[[] for cluster in range(numberOfCluster)] for whale in range(numberOfWhale)]\n",
    "    dataPointsInCluster=np.zeros((numberOfWhale,numberOfCluster))\n",
    "    bestWhale=0\n",
    "    bestDist=float(\"inf\")\n",
    "    startTime=time.time()\n",
    "    for whale in range(numberOfWhale):\n",
    "        dist=0.00\n",
    "        clus1=np.zeros((features))\n",
    "        clus2=np.zeros((features))\n",
    "        for dataPoint in range(dataPoints):\n",
    "            bestEuclidianDist=float(\"inf\")\n",
    "            bestCluster=0\n",
    "            for cluster in range(numberOfCluster):\n",
    "                euclidDist=np.linalg.norm(centresOfwhale[whale,cluster]-X[dataPoint,:])\n",
    "                if(euclidDist<bestEuclidianDist):\n",
    "                    bestEuclidianDist=euclidDist\n",
    "                    bestCluster=cluster\n",
    "            dist=dist+bestEuclidianDist\n",
    "            #dataPointsInCluster[whale][bestCluster].append(dataPoint)\n",
    "            dataPointsInCluster[whale][bestCluster]=dataPointsInCluster[whale][bestCluster]+1\n",
    "            if bestCluster==0:\n",
    "                clus1=clus1+X[dataPoint]\n",
    "            if bestCluster==1:\n",
    "                clus2=clus2+X[dataPoint]\n",
    "        if(dist<bestDist):\n",
    "            bestDist=dist\n",
    "            bestWhale=whale\n",
    "        \n",
    "        if(dataPointsInCluster[whale][0]!=0):\n",
    "            centresOfwhale[whale][0]=clus1/dataPointsInCluster[whale][0]\n",
    "        \n",
    "        if(dataPointsInCluster[whale][1]!=0):\n",
    "            centresOfwhale[whale][1]=clus1/dataPointsInCluster[whale][1]\n",
    "    #shift the centroid in the centre of the datapoints inside cluster\n",
    "\n",
    "    '''    \n",
    "    print(time.time()-startTime)\n",
    "    startTime=time.time();\n",
    "    print(\"shift started\")\n",
    "    for whale in range(numberOfWhale):\n",
    "        for cluster in range(numberOfCluster):\n",
    "            numberOfPoint=len(dataPointsInCluster[whale][cluster])\n",
    "            if numberOfPoint==0 :\n",
    "                continue\n",
    "            for feature in range(features):\n",
    "                sum=0.00\n",
    "                for dataPoint in dataPointsInCluster[whale][cluster]:\n",
    "                    sum=sum+X[dataPoint][feature];\n",
    "                sum=sum/numberOfPoint\n",
    "                centresOfwhale[whale,cluster,feature]=sum'''\n",
    "    \n",
    "    #shift centroids using equations of WOA\n",
    "#     print(time.time()-startTime)\n",
    "    startTime=time.time();\n",
    "#     print(\"woa started\")\n",
    "    a=2-iteration*((2.00)/iterations) #eqn 2.3\n",
    "    a2=-1+iteration*((-1.00)/iterations)\n",
    "    for whale in range(numberOfWhale):\n",
    "        r1=random.random()\n",
    "        r2=random.random()\n",
    "        A=2*a*r1-a;  # Eq. (2.3) in the paper\n",
    "        C=2*r2;      # Eq. (2.4) in the paper\n",
    "        p=random.random()\n",
    "        b=1;               #  parameters in Eq. (2.5)\n",
    "        l=(a2-1)*random.random()+1;   #  parameters in Eq. (2.5)\n",
    "        \n",
    "        for cluster in range(numberOfCluster):\n",
    "            if p<0.5 :\n",
    "                if abs(A)>=1 :\n",
    "                    rand_leader_index = int(math.floor((numberOfWhale-1)*random.random()+1));\n",
    "                    X_rand = centresOfwhale[rand_leader_index]\n",
    "                    D_X_rand=abs(C*X_rand[cluster]-centresOfwhale[whale,cluster]); # Eq. (2.7)\n",
    "                    centresOfwhale[whale,cluster]=X_rand[cluster]-A*D_X_rand;      # Eq. (2.8)\n",
    "                elif abs(A)<1 :\n",
    "                    D_Leader=abs(C*centresOfwhale[bestWhale,cluster]-centresOfwhale[whale,cluster]); # Eq. (2.1)\n",
    "                    centresOfwhale[whale,cluster]=centresOfwhale[bestWhale,cluster]-A*D_Leader;      # Eq. (2.2)\n",
    "            elif p>=0.5 :\n",
    "                distance2Leader=abs(centresOfwhale[bestWhale,cluster]-centresOfwhale[whale,cluster]);      # Eq. (2.5)\n",
    "                centresOfwhale[whale,cluster]=distance2Leader*math.exp(b*l)*math.cos(l*2*3.14)+centresOfwhale[bestWhale,cluster];\n",
    "#     print(time.time()-startTime)\n",
    "    startTime=time.time();\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix(y,y_pred)\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(accuracy_score(y,y_pred))\n",
    "\n",
    "WOACluster=[0 for dataPoint in range(dataPoints)]\n",
    "for dataPoint in range(dataPoints):\n",
    "    d1=np.linalg.norm(centresOfwhale[bestWhale,0]-X[dataPoint,:])\n",
    "    d2=np.linalg.norm(centresOfwhale[bestWhale,1]-X[dataPoint,:])\n",
    "    d3=np.linalg.norm(centresOfwhale[bestWhale,2]-X[dataPoint,:])\n",
    "#     print(d1,d2, d3)\n",
    "    if d1<d2 and d1<d3 :\n",
    "        WOACluster[dataPoint]=0\n",
    "    elif d2<d1 and d2<d3:\n",
    "        WOACluster[dataPoint]=1\n",
    "    elif d3<d1 and d3<d2:\n",
    "        WOACluster[dataPoint]=2\n",
    "            \n",
    "print(WOACluster)\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix(y,y_pred1)\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(accuracy_score(y,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAKMeans_Sil = metrics.silhouette_score(X, WOACluster, metric='euclidean')\n",
    "GAKMeans_Sil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "colors = np.array(['g', 'r', 'b', 'c', 'k', 'y','royalblue', 'maroon', 'forestgreen',\n",
    "                   'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
